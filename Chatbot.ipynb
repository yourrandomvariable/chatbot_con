{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYMXTM-Ks--j"
   },
   "source": [
    "<h1 align='center'>Conversational AI Chatbot</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paZPgO4ctLYs"
   },
   "source": [
    "personal project - Mrigank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BkUzz_SQtMQs",
    "outputId": "b2b24083-3795-467d-bc7e-d06c272f2af6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKnhzhRwuBuv"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
    "import pandas as pd\n",
    "import pickle\n",
    "print( tf.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UATBeb3quFEw"
   },
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pg2ZHin7tu4K"
   },
   "outputs": [],
   "source": [
    "data_path='chatbot.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wL2gv3tUuJrR"
   },
   "source": [
    "since data path is a directory to access it i  have used the following lines to extract it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLZnaQYKtxwe"
   },
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(600, len(lines) - 1)]:\n",
    "    input_text = line.split('\\t')[0]\n",
    "    target_text = line.split('\\t')[1]\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwA2vzZft0EI"
   },
   "outputs": [],
   "source": [
    "zippedList =  list(zip(input_texts, target_texts))\n",
    "lines = pd.DataFrame(zippedList, columns = ['input' , 'output']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXPh83iFuVLT"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysLAwL3buP6b"
   },
   "source": [
    "since the chatbot is train over seq2seq model as the data is sequential\n",
    "for seq2seq encoder-decoder will best choice to train chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfZOsjseuWSi"
   },
   "source": [
    "## Preparing input data for the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EH57Ze2tt1pd",
    "outputId": "fd0632e9-9928-43eb-c544-ac129870d58a"
   },
   "outputs": [],
   "source": [
    "input_lines = list()\n",
    "for line in lines.input:\n",
    "    input_lines.append( line ) \n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( input_lines ) \n",
    "tokenized_input_lines = tokenizer.texts_to_sequences( input_lines ) \n",
    "\n",
    "length_list = list()\n",
    "for token_seq in tokenized_input_lines:\n",
    "    length_list.append( len( token_seq ))\n",
    "max_input_length = np.array( length_list ).max()\n",
    "print( 'Input max length is {}'.format( max_input_length ))\n",
    "\n",
    "padded_input_lines = preprocessing.sequence.pad_sequences( tokenized_input_lines , maxlen=max_input_length , padding='post' )\n",
    "encoder_input_data = np.array( padded_input_lines )\n",
    "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n",
    "\n",
    "input_word_dict = tokenizer.word_index\n",
    "num_input_tokens = len( input_word_dict )+1\n",
    "print( 'Number of Input tokens = {}'.format( num_input_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H36dVb-8t3A8",
    "outputId": "02088351-2fa2-485d-b804-e6db9e58a4a3"
   },
   "outputs": [],
   "source": [
    "encoder_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c4qRVvkuftP"
   },
   "source": [
    "## Preparing input data for the Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5L0Frq11ucRs",
    "outputId": "08111419-3fdd-43ba-bd5b-dcf3056a0d7e"
   },
   "outputs": [],
   "source": [
    "output_lines = list()\n",
    "for line in lines.output:\n",
    "    output_lines.append( '<START> ' + line + ' <END>' )  \n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( output_lines ) \n",
    "tokenized_output_lines = tokenizer.texts_to_sequences( output_lines ) \n",
    "\n",
    "length_list = list()\n",
    "for token_seq in tokenized_output_lines:\n",
    "    length_list.append( len( token_seq ))\n",
    "max_output_length = np.array( length_list ).max()\n",
    "print( 'Output max length is {}'.format( max_output_length ))\n",
    "\n",
    "padded_output_lines = preprocessing.sequence.pad_sequences( tokenized_output_lines , maxlen=max_output_length, padding='post' )\n",
    "decoder_input_data = np.array( padded_output_lines )\n",
    "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
    "\n",
    "output_word_dict = tokenizer.word_index\n",
    "num_output_tokens = len( output_word_dict )+1\n",
    "print( 'Number of Output tokens = {}'.format( num_output_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vApNQbjmumbg"
   },
   "source": [
    "## Preparing target data for the Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "slPD0xbyuj76",
    "outputId": "b0ba24e2-1bf2-4bce-fb23-7c59832545f4"
   },
   "outputs": [],
   "source": [
    "decoder_target_data = list()\n",
    "for token_seq in tokenized_output_lines:\n",
    "    decoder_target_data.append( token_seq[ 1 : ] ) \n",
    "    \n",
    "padded_output_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
    "onehot_output_lines = utils.to_categorical( padded_output_lines , num_output_tokens )\n",
    "decoder_target_data = np.array(onehot_output_lines )\n",
    "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Vygy5mfussc"
   },
   "source": [
    "## Defining the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QxTbCfozupeK",
    "outputId": "9b4699f3-fbf9-4749-b122-e9074f36de79"
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( num_input_tokens, 256 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 256 , return_state=True , recurrent_dropout=0.2 , dropout=0.2 )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( num_output_tokens, 256 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( num_output_tokens , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ISrFx4ORuvo-",
    "outputId": "f17ac8b9-08c9-4aa5-afc3-3dbc95f212cc"
   },
   "outputs": [],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=124, epochs=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J5_wTj6Au0zY",
    "outputId": "6f589d0c-813d-4860-9312-c8d0b1fff1e0"
   },
   "outputs": [],
   "source": [
    "!pip install pyngrok==4.1.1\n",
    "!pip install flask_ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTVpvxfvzwRW",
    "outputId": "dbf05a1b-a2b1-48c0-bd78-cbbe66fd987e"
   },
   "outputs": [],
   "source": [
    "!ngrok authtoken \"Enter your Ngrok key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tdgq4ApCz0Cx"
   },
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=(256,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=(256,))\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "\n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZ9dJanOz3w2"
   },
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( input_word_dict[ word ] )\n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n",
    "\n",
    "\n",
    "enc_model , dec_model = make_inference_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6mlFGe1z5xd",
    "outputId": "613a671e-f778-4a7b-90f9-b311108f7673"
   },
   "outputs": [],
   "source": [
    "from flask import Flask,render_template,request\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask\n",
    "app=Flask(__name__)\n",
    "\n",
    "run_with_ngrok(app)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "def predict_placement():\n",
    "    text = str(request.form.get('uname'))\n",
    "    print(text)\n",
    "    states_values = enc_model.predict(str_to_tokens(text))\n",
    "    empty_target_seq = np.zeros((1, 1))\n",
    "    empty_target_seq[0, 0] = output_word_dict['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition:\n",
    "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "        sampled_word = None\n",
    "        for word, index in output_word_dict.items():\n",
    "            if sampled_word_index == index:\n",
    "                decoded_translation += ' {}'.format(word)\n",
    "                sampled_word = word\n",
    "\n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        empty_target_seq = np.zeros((1, 1))\n",
    "        empty_target_seq[0, 0] = sampled_word_index\n",
    "        states_values = [h, c]\n",
    "\n",
    "    result= decoded_translation.replace(' end', '')\n",
    "    return render_template('index.html',result=\"Bot: {}\".format(result))\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5InbWb6z8-p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
